{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZ2yB1c6i/D66f/iiwvlsU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonemking/at-challenges/blob/challenge-0/at_sr_challenge0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqrE2McDZe0-"
      },
      "outputs": [],
      "source": [
        "# Challenge 0: Understanding Attention\n",
        "# This notebook is designed to help you understand the key concepts of the paper\n",
        "# \"Attention Is All You Need\". Your task is to implement the core components of\n",
        "# the attention mechanism and experiment with it.\n",
        "\n",
        "## 1. Introduction\n",
        "\"\"\"\n",
        "The \"Attention Is All You Need\" paper introduced the Transformer model, which\n",
        "has since revolutionized natural language processing (NLP) and other machine\n",
        "learning fields. At the heart of this model is the attention mechanism, which\n",
        "allows the model to focus on different parts of the input sequence dynamically.\n",
        "\n",
        "In this challenge, you'll implement a simplified version of the attention\n",
        "mechanism and apply it to some test data. Follow the TODO sections to fill\n",
        "in the code.\n",
        "\"\"\"\n",
        "\n",
        "## 2. Step 1: Setup & Imports\n",
        "# First, let's import the necessary libraries for our task.\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# TODO: Explore any additional libraries that might be useful for visualizing the results.\n",
        "\n",
        "## 3. Step 2: Defining the Attention Mechanism\n",
        "\"\"\"\n",
        "In the attention mechanism, we compute the attention scores between\n",
        "a query and a set of key-value pairs. The attention scores are\n",
        "used to compute a weighted sum of the values based on the relevance of the keys to the query.\n",
        "\n",
        "The formula for dot-product attention is:\n",
        "    Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V\n",
        "\"\"\"\n",
        "\n",
        "# Here, Q = query, K = keys, and V = values.\n",
        "# d_k is the dimension of the key/query vectors.\n",
        "\n",
        "def attention(query, key, value, d_k):\n",
        "    \"\"\"\n",
        "    Computes the dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        query: A tensor of shape (batch_size, seq_len, d_k)\n",
        "        key: A tensor of shape (batch_size, seq_len, d_k)\n",
        "        value: A tensor of shape (batch_size, seq_len, d_v)\n",
        "        d_k: Dimension of the keys/queries.\n",
        "\n",
        "    Returns:\n",
        "        output: The result of the attention mechanism.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Implement the attention mechanism.\n",
        "    # Step 1: Compute the dot product of the query and key, and scale by sqrt(d_k)\n",
        "    attention_scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)\n",
        "\n",
        "    # TODO: Step 2: Apply softmax to get the attention weights\n",
        "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "    # TODO: Step 3: Use the attention weights to compute a weighted sum of the values\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# TODO: Test the function with random inputs to ensure it works as expected.\n",
        "\n",
        "## 4. Step 3: Applying Attention\n",
        "\"\"\"\n",
        "Now that you have implemented the attention mechanism, let's apply it to some test data.\n",
        "In a real-world scenario, attention mechanisms are used to enhance sequence-to-sequence\n",
        "models, such as those used in machine translation.\n",
        "\"\"\"\n",
        "\n",
        "# Let's create some random data to simulate queries, keys, and values.\n",
        "# TODO: Generate random tensors for query, key, and value\n",
        "query = torch.rand(5, 10, 64)  # Example: 5 sequences, 10 tokens per sequence, 64-dimensional embeddings\n",
        "key = torch.rand(5, 10, 64)\n",
        "value = torch.rand(5, 10, 64)\n",
        "\n",
        "# Use the attention mechanism to compute the output\n",
        "d_k = query.size(-1)\n",
        "output, attention_weights = attention(query, key, value, d_k)\n",
        "\n",
        "# TODO: Print the output and attention weights for inspection.\n",
        "\n",
        "## 5. Step 4: Testing & Visualization\n",
        "\"\"\"\n",
        "To fully understand how attention works, it's useful to visualize the attention\n",
        "weights. The attention weights tell us which parts of the input sequence the\n",
        "model is focusing on at each step.\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Visualize the attention weights using matplotlib or any other library of your choice.\n",
        "# You can plot the attention weights as a heatmap to see how the model focuses on different tokens.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def visualize_attention(attention_weights):\n",
        "    \"\"\"\n",
        "    Visualizes the attention weights as a heatmap.\n",
        "    \"\"\"\n",
        "    attention_weights_np = attention_weights.detach().numpy()\n",
        "    sns.heatmap(attention_weights_np[0], annot=True, cmap=\"Blues\")\n",
        "    plt.show()\n",
        "\n",
        "# TODO: Test the visualization function with the attention weights computed above.\n",
        "\n",
        "visualize_attention(attention_weights)\n",
        "\n",
        "## 6. Conclusion\n",
        "\"\"\"\n",
        "Congratulations! You've just implemented the core of the attention mechanism,\n",
        "a key component in modern neural networks like the Transformer model.\n",
        "\n",
        "Feel free to experiment further by modifying the input data or adjusting the\n",
        "dimension sizes. Understanding this mechanism is an important step toward\n",
        "grasping more advanced models, such as BERT and GPT.\n",
        "\"\"\"\n"
      ]
    }
  ]
}